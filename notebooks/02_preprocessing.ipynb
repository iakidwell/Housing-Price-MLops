{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50e9a471-2d98-47d3-a5df-bb113ae88ccf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Preprocessing for Ames Housing Dataset\n",
    "\n",
    "## Prepare the Ames Housing Data for AutoML and final modeling.\n",
    "\n",
    "## 1. Transforming the target variable (SalePrice)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23c486f2-b2b4-40ae-9dc5-8452c1b7c38a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Prepared by Ian Kidwell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c33218d-39f0-46ee-ad93-a52dc13a79dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2b933c7-7cbb-47a2-af9e-42773e1425c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_path = \"../data/AmesHousing.csv\"\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Quick check\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c110c42c-8d5f-48a7-9743-7e6bd1f36e37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Log-transform the target variable\n",
    "df['SalePrice_log'] = np.log1p(df['SalePrice'])  # log1p handles zero values safely\n",
    "\n",
    "# Quick visualization to see the effect\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "sns.histplot(df['SalePrice'], kde=True, bins=50)\n",
    "plt.title('Original SalePrice Distribution')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.histplot(df['SalePrice_log'], kde=True, bins=50, color='orange')\n",
    "plt.title('Log-Transformed SalePrice Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26f5228d-2b7c-4337-a16e-895717f31086",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "784262f9-6e6b-4db1-afb8-e31b51173ec8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Identify numeric and categorical columns\n",
    "numeric_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Optional: remove target variable from features\n",
    "if 'SalePrice' in numeric_features:\n",
    "    numeric_features.remove('SalePrice')\n",
    "\n",
    "# Quick check\n",
    "print(\"Numeric features:\", numeric_features[:10], \"...\")\n",
    "print(\"Categorical features:\", categorical_features[:10], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05557733-f198-47dc-95f1-29090be30673",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# For numeric features\n",
    "for col in numeric_features:\n",
    "    df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "# For categorical features\n",
    "for col in categorical_features:\n",
    "    df[col] = df[col].fillna(\"Missing\")\n",
    "\n",
    "\n",
    "# Quick check to confirm\n",
    "missing_summary = df.isnull().sum()[df.isnull().sum() > 0]\n",
    "print(\"Remaining missing values:\\n\", missing_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12346333-71bf-4199-a323-7d4d8b9f925a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Categorical Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8aa94a4-f326-4451-9d82-bb3855fd16a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List all categorical columns\n",
    "for col in categorical_features:\n",
    "    print(f\"{col}: {df[col].unique()}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e360c5ab-7f91-400f-b377-80fbecb6c747",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Define ordinal mappings\n",
    "qual_mapping = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'Missing': 0}\n",
    "bsmt_exposure_mapping = {'No': 0, 'Mn': 1, 'Av': 2, 'Gd': 3, 'Missing': 0}\n",
    "bsmt_fin_mapping = {'Unf': 0, 'LwQ': 1, 'Rec': 2, 'BLQ': 3, 'ALQ': 4, 'GLQ': 5, 'Missing': 0}\n",
    "garage_finish_mapping = {'Missing': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3}\n",
    "paved_drive_mapping = {'N': 0, 'P': 1, 'Y': 2}\n",
    "functional_mapping = {'Sal':1, 'Sev':2, 'Maj2':3, 'Maj1':4, 'Mod':5, 'Min2':6, 'Min1':7, 'Typ':8}\n",
    "\n",
    "# Apply ordinal encoding\n",
    "ordinal_features = ['Exter Qual', 'Exter Cond', 'Bsmt Qual', 'Bsmt Cond', 'Bsmt Exposure',\n",
    "                    'BsmtFin Type 1', 'BsmtFin Type 2', 'Heating QC', 'Kitchen Qual',\n",
    "                    'Fireplace Qu', 'Garage Finish', 'Garage Qual', 'Garage Cond',\n",
    "                    'Pool QC', 'Functional', 'Paved Drive']\n",
    "\n",
    "df['Exter Qual'] = df['Exter Qual'].map(qual_mapping)\n",
    "df['Exter Cond'] = df['Exter Cond'].map(qual_mapping)\n",
    "df['Bsmt Qual'] = df['Bsmt Qual'].map(qual_mapping)\n",
    "df['Bsmt Cond'] = df['Bsmt Cond'].map(qual_mapping)\n",
    "df['Bsmt Exposure'] = df['Bsmt Exposure'].map(bsmt_exposure_mapping)\n",
    "df['BsmtFin Type 1'] = df['BsmtFin Type 1'].map(bsmt_fin_mapping)\n",
    "df['BsmtFin Type 2'] = df['BsmtFin Type 2'].map(bsmt_fin_mapping)\n",
    "df['Heating QC'] = df['Heating QC'].map(qual_mapping)\n",
    "df['Kitchen Qual'] = df['Kitchen Qual'].map(qual_mapping)\n",
    "df['Fireplace Qu'] = df['Fireplace Qu'].map(qual_mapping)\n",
    "df['Garage Finish'] = df['Garage Finish'].map(garage_finish_mapping)\n",
    "df['Garage Qual'] = df['Garage Qual'].map(qual_mapping)\n",
    "df['Garage Cond'] = df['Garage Cond'].map(qual_mapping)\n",
    "df['Pool QC'] = df['Pool QC'].map(qual_mapping)\n",
    "df['Functional'] = df['Functional'].map(functional_mapping)\n",
    "df['Paved Drive'] = df['Paved Drive'].map(paved_drive_mapping)\n",
    "\n",
    "# Remaining categorical features â†’ one-hot encode\n",
    "nominal_features = [col for col in categorical_features if col not in ordinal_features]\n",
    "df = pd.get_dummies(df, columns=nominal_features, drop_first=True)\n",
    "\n",
    "# Quick check\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2bd751a-05b2-4f6b-b073-193cb3f38a42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Scaling Numeric Features (for AutoML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "466a31e8-f5dc-4c0c-b2d0-2e30b5ee4b34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Create an unscaled copy\n",
    "df_unscaled = df.copy()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform numeric features\n",
    "df[numeric_features] = scaler.fit_transform(df[numeric_features])\n",
    "\n",
    "# Quick check\n",
    "df[numeric_features].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ba977f1-9da7-4781-8c28-7da1387a07f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0784106f-4de4-4dc2-bb2c-3408d9c5b8c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List all features in the dataset\n",
    "all_features = df.columns.tolist()\n",
    "print(\"Total features:\", len(all_features))\n",
    "print(all_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20c8a5b0-48a8-4284-ad96-e9072198173f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Drop only non-informative columns (Keeping your logic)\n",
    "#df.drop(columns=['Order', 'PID'], inplace=True)\n",
    "#df_unscaled.drop(columns=['Order', 'PID'], inplace=True)\n",
    "\n",
    "# --- SAVE TO DATA FOLDER ---\n",
    "\n",
    "# We add '../data/' to the filename to send it to the right folder\n",
    "path_scaled = \"../data/ames_preprocessed.csv\"\n",
    "path_unscaled = \"../data/ames_preprocessed_unscaled.csv\"\n",
    "\n",
    "# Save preprocessed dataset for AutoML\n",
    "df.to_csv(path_scaled, index=False)\n",
    "df_unscaled.to_csv(path_unscaled, index=False)\n",
    "\n",
    "# Verification: Print exactly where they were saved\n",
    "print(f\"Saved file to: {os.path.abspath(path_scaled)}\")\n",
    "print(f\"Saved file to: {os.path.abspath(path_unscaled)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc9e829e-6357-4e10-bb06-e331d9463d32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Drop all text/object columns (Logic stays the same)\n",
    "df_numeric_only = df.select_dtypes(include=['int64', 'float64']).copy()\n",
    "df_numeric_only_unscaled = df_unscaled.select_dtypes(include=['int64', 'float64']).copy()\n",
    "\n",
    "# Quick check\n",
    "print(df_numeric_only.info())\n",
    "\n",
    "# --- FIX: Save to the data folder ---\n",
    "# We define the paths relative to this notebook (go up one level, then into data)\n",
    "path_numeric = \"../data/ames_preprocessed_numeric.csv\"\n",
    "path_numeric_unscaled = \"../data/ames_preprocessed_numeric_unscaled.csv\"\n",
    "\n",
    "# Save the cleaned DataFrames\n",
    "df_numeric_only.to_csv(path_numeric, index=False)\n",
    "df_numeric_only_unscaled.to_csv(path_numeric_unscaled, index=False)\n",
    "\n",
    "# Verification: Print the full path so you can confirm they landed in the right spot\n",
    "print(f\"Saved numeric-only CSV to: {os.path.abspath(path_numeric)}\")\n",
    "print(f\"Saved unscaled CSV to:     {os.path.abspath(path_numeric_unscaled)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97e2b3e9-d5f2-41cc-a46d-4b8b1bf0b541",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# --- INPUT: Read from ../data/ ---\n",
    "# This matches the file we saved in the previous cell\n",
    "input_path = \"../data/ames_preprocessed_numeric.csv\"\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# Split into train and test\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- OUTPUT: Save to ../data/ ---\n",
    "train_path = \"../data/train.csv\"\n",
    "test_path = \"../data/test.csv\"\n",
    "\n",
    "train_df.to_csv(train_path, index=False)\n",
    "test_df.to_csv(test_path, index=False)\n",
    "\n",
    "# Verification\n",
    "print(f\"Read data from: {os.path.abspath(input_path)}\")\n",
    "print(f\"Saved Train to: {os.path.abspath(train_path)}\")\n",
    "print(f\"Saved Test to:  {os.path.abspath(test_path)}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_preprocessing",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
