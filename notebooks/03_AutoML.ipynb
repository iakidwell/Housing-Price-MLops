{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "935d95c4-b113-4cf6-9eff-70990d471b72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flaml in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f178ceaa-b81d-4ea0-b805-1e39aee9d364/lib/python3.12/site-packages (2.3.6)\nRequirement already satisfied: NumPy>=1.17 in /databricks/python3/lib/python3.12/site-packages (from flaml) (2.1.3)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install flaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26db10e0-b217-4c71-ace2-bb9b973caacd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tried to attach usage logger `pyspark.databricks.pandas.usage_logger`, but an exception was raised: JVM wasn't initialised. Did you call it on executor side?\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-6926184878885630>, line 5\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Install FLAML in your Databricks cluster first:\u001B[39;00m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# %pip install flaml\u001B[39;00m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n",
       "\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mflaml\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoML\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m train_test_split\n",
       "\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Load the numeric-only dataset\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mImportError\u001B[0m: cannot import name 'AutoML' from 'flaml' (/local_disk0/.ephemeral_nfs/envs/pythonEnv-30e08121-9934-439f-819f-b9b62d39a608/lib/python3.12/site-packages/flaml/__init__.py)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ImportError",
        "evalue": "cannot import name 'AutoML' from 'flaml' (/local_disk0/.ephemeral_nfs/envs/pythonEnv-30e08121-9934-439f-819f-b9b62d39a608/lib/python3.12/site-packages/flaml/__init__.py)"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>ImportError</span>: cannot import name 'AutoML' from 'flaml' (/local_disk0/.ephemeral_nfs/envs/pythonEnv-30e08121-9934-439f-819f-b9b62d39a608/lib/python3.12/site-packages/flaml/__init__.py)\n[Trace ID: 00-3f7f1467fec82efaecbe0cb5bb390dbd-c2780040cfe7c80e-00]"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "NOTEBOOK_USER_ERROR",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "pysparkSummary": null,
        "sqlState": "KD00G",
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
        "File \u001B[0;32m<command-6926184878885630>, line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Install FLAML in your Databricks cluster first:\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# %pip install flaml\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mflaml\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoML\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m train_test_split\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Load the numeric-only dataset\u001B[39;00m\n",
        "\u001B[0;31mImportError\u001B[0m: cannot import name 'AutoML' from 'flaml' (/local_disk0/.ephemeral_nfs/envs/pythonEnv-30e08121-9934-439f-819f-b9b62d39a608/lib/python3.12/site-packages/flaml/__init__.py)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Install FLAML in the Databricks cluster first:\n",
    "# %pip install flaml\n",
    "\n",
    "import pandas as pd\n",
    "from flaml import AutoML\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the numeric-only dataset\n",
    "df = pd.read_csv(\"ames_preprocessed_numeric.csv\")\n",
    "\n",
    "# Split features/target\n",
    "target = \"SalePrice_log\"  # replace with your target column\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# AutoML setup\n",
    "automl = AutoML()\n",
    "automl_settings = {\n",
    "    \"time_budget\": 300,  # 5 minutes max\n",
    "    \"metric\": 'rmse',\n",
    "    \"task\": 'regression',\n",
    "    \"log_file_name\": \"flaml.log\",\n",
    "}\n",
    "\n",
    "# Train\n",
    "automl.fit(X_train=X_train, y_train=y_train, **automl_settings)\n",
    "\n",
    "# Best model & prediction\n",
    "print(\"Best ML learner:\", automl.best_estimator)\n",
    "y_pred = automl.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Test RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43a2735d-974b-463b-ac38-7714472e54bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "AutoML Ames",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}