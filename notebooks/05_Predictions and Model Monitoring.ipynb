{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7970877-d584-44ce-a554-a921251bc2e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Notebook 5: Inferencing with Deployed Model\n",
    "\n",
    "### In this notebook, we deploy the model previously logged to MLflow and make some predictions to demonstrate that it works. We also transform some of the data in the test set to practice model monitoring in Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cba4e75d-ffaa-47ac-908a-227415e87fdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded for inference!\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Load registered model\n",
    "# ----------------------------\n",
    "model_name = \"rf_ames_model\"\n",
    "model_version = 1 \n",
    "\n",
    "# Construct the MLflow model URI\n",
    "model_uri = f\"models:/{model_name}/{model_version}\"\n",
    "\n",
    "# Load the model\n",
    "loaded_model = mlflow.sklearn.load_model(model_uri)\n",
    "print(\"Model loaded for inference!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f5195ba-5571-4cbc-b291-d1b1c15bca57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: (586, 53), y_test shape: (586,)\n   MS SubClass  Lot Frontage  Lot Area  ...  Mo Sold  Yr Sold  SalePrice\n0           70          68.0      5100  ...        6     2008     161000\n1          160          21.0      1890  ...        7     2006     116000\n2           60          62.0      7162  ...        5     2006     196500\n3           20          60.0      8070  ...        8     2007     123600\n4           30          50.0      7000  ...        7     2008     126000\n\n[5 rows x 53 columns]\n0    11.989166\n1    11.661354\n2    12.188423\n3    11.724814\n4    11.744045\nName: SalePrice_log, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 2. Load test data for evaluation\n",
    "# ----------------------------\n",
    "\n",
    "# Paths to the saved test set\n",
    "X_test_path = \"/Workspace/Users/iakidwell@uchicago.edu/X_test.csv\"\n",
    "y_test_path = \"/Workspace/Users/iakidwell@uchicago.edu/y_test.csv\"\n",
    "\n",
    "# Load test features and target\n",
    "X_test = pd.read_csv(X_test_path)\n",
    "y_test = pd.read_csv(y_test_path).squeeze()  # squeeze to convert to Series\n",
    "\n",
    "# Quick check\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "print(X_test.head())\n",
    "print(y_test.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c63f1ab8-c996-4004-a5c0-29e981d6bb59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE on deployed model: 1877.22\nExample 1 predicted SalePrice: $161144.92\nExample 2 predicted SalePrice: $115959.14\nExample 3 predicted SalePrice: $196642.56\nExample 4 predicted SalePrice: $123971.65\nExample 5 predicted SalePrice: $125313.19\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Make predictions on test set\n",
    "# ----------------------------\n",
    "preds_log = loaded_model.predict(X_test)\n",
    "\n",
    "# Invert the log transform to get SalePrice\n",
    "preds = np.expm1(preds_log)  # converts log(SalePrice+1) back to SalePrice\n",
    "y_test_actual = np.expm1(y_test)  # convert actual log values back\n",
    "\n",
    "# Calculate RMSE manually\n",
    "rmse = np.sqrt(mean_squared_error(y_test_actual, preds))\n",
    "print(f\"Test RMSE on deployed model: {rmse:.2f}\")\n",
    "\n",
    "# Display the first 5 predictions\n",
    "for i, price in enumerate(preds[:5], 1):\n",
    "    print(f\"Example {i} predicted SalePrice: ${price:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ff1bb99-7e4d-4e72-a613-7bacb575f009",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged test evaluation to MLflow run 0136aeb20deb4287b5ce0c72f6be7fc4\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "# Absolute path for your experiment\n",
    "experiment_path = \"/Users/iakidwell@uchicago.edu/ames_housing_test_eval\"\n",
    "\n",
    "# Create experiment if it doesn't exist\n",
    "experiment = mlflow.get_experiment_by_name(experiment_path)\n",
    "if experiment is None:\n",
    "    experiment_id = mlflow.create_experiment(experiment_path)\n",
    "else:\n",
    "    experiment_id = experiment.experiment_id\n",
    "\n",
    "# Start a run in this experiment\n",
    "with mlflow.start_run(run_name=\"deployed_model_test_eval\", experiment_id=experiment_id) as run:\n",
    "    # Log metric\n",
    "    mlflow.log_metric(\"test_rmse\", rmse)\n",
    "\n",
    "    # Save predictions and log as artifact\n",
    "    eval_df = pd.DataFrame({\n",
    "        \"y_true\": y_test_actual,\n",
    "        \"y_pred\": preds\n",
    "    })\n",
    "    eval_csv = \"test_predictions.csv\"\n",
    "    eval_df.to_csv(eval_csv, index=False)\n",
    "    mlflow.log_artifact(eval_csv)\n",
    "\n",
    "    print(f\"Logged test evaluation to MLflow run {run.info.run_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13de3ae6-6fe0-4abe-8875-a35642c6ea47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Feature Changes\n",
    "\n",
    "## This is done to test out the modeling monitoring dashboard in Databricks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc0497af-bf37-4802-9e96-2f0c4e9e77e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features in the dataset:\n1. MS SubClass\n2. Lot Frontage\n3. Lot Area\n4. Overall Qual\n5. Overall Cond\n6. Year Built\n7. Year Remod/Add\n8. Mas Vnr Area\n9. Exter Qual\n10. Exter Cond\n11. Bsmt Qual\n12. Bsmt Cond\n13. Bsmt Exposure\n14. BsmtFin Type 1\n15. BsmtFin SF 1\n16. BsmtFin Type 2\n17. BsmtFin SF 2\n18. Bsmt Unf SF\n19. Total Bsmt SF\n20. Heating QC\n21. 1st Flr SF\n22. 2nd Flr SF\n23. Low Qual Fin SF\n24. Gr Liv Area\n25. Bsmt Full Bath\n26. Bsmt Half Bath\n27. Full Bath\n28. Half Bath\n29. Bedroom AbvGr\n30. Kitchen AbvGr\n31. Kitchen Qual\n32. TotRms AbvGrd\n33. Functional\n34. Fireplaces\n35. Fireplace Qu\n36. Garage Yr Blt\n37. Garage Finish\n38. Garage Cars\n39. Garage Area\n40. Garage Qual\n41. Garage Cond\n42. Paved Drive\n43. Wood Deck SF\n44. Open Porch SF\n45. Enclosed Porch\n46. 3Ssn Porch\n47. Screen Porch\n48. Pool Area\n49. Pool QC\n50. Misc Val\n51. Mo Sold\n52. Yr Sold\n53. SalePrice\n"
     ]
    }
   ],
   "source": [
    "# List all features in X_test\n",
    "print(\"Features in the dataset:\")\n",
    "for i, col in enumerate(X_test.columns, 1):\n",
    "    print(f\"{i}. {col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "674a3a88-b9a1-4624-9824-f90bedac125d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE on mutated test data: 1877.41\nLogged RMSE to MLflow run 38834ece25b94b8f943264e34a2c489a\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import mlflow\n",
    "\n",
    "# Make a copy of X_test so we don't overwrite original test data\n",
    "X_test_mutated = X_test.copy()\n",
    "\n",
    "# Specify the features to modify\n",
    "feature_1 = \"Lot Area\"\n",
    "feature_2 = \"Fireplaces\"\n",
    "\n",
    "# Mutate the features: random perturbation\n",
    "np.random.seed(42)  # for reproducibility\n",
    "X_test_mutated[feature_1] = X_test_mutated[feature_1] + np.random.normal(0, 1, size=X_test_mutated.shape[0])\n",
    "X_test_mutated[feature_2] = X_test_mutated[feature_2] + np.random.normal(0, 1, size=X_test_mutated.shape[0])\n",
    "\n",
    "# Predict with the deployed model\n",
    "mutated_preds_log = loaded_model.predict(X_test_mutated)\n",
    "mutated_preds = np.expm1(mutated_preds_log)  # invert log-transform\n",
    "\n",
    "# Evaluate RMSE on mutated data (manual square root)\n",
    "y_test_actual = np.expm1(y_test)\n",
    "mutated_rmse = mean_squared_error(y_test_actual, mutated_preds) ** 0.5\n",
    "print(f\"Test RMSE on mutated test data: {mutated_rmse:.2f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Log evaluation with MLflow\n",
    "# ----------------------------\n",
    "experiment_path = \"/Users/iakidwell@uchicago.edu/ames_housing_test_eval\"  # update with your workspace path\n",
    "mlflow.set_experiment(experiment_path)\n",
    "\n",
    "with mlflow.start_run(run_name=\"mutated_test_eval\") as run:\n",
    "    mlflow.log_metric(\"mutated_test_rmse\", mutated_rmse)\n",
    "    print(f\"Logged RMSE to MLflow run {run.info.run_id}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_Predictions and Model Monitoring",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}